{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "policy_gradient_cartpole_tf2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1NReiefN73sxuz6NYIw90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/floatoak/fundamentals_of_deep_learning/blob/main/9_deep_reinforcement_learning/policy_gradient_cartpole_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zif0FjmIPrAo"
      },
      "source": [
        "# Implementing Pole-Cart with Policy Gradients in Tensorflow 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyqSamUbRbbL"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from typing import Callable"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rndM-aqHP5oI"
      },
      "source": [
        "## Creating an Agent\n",
        "Define a class PolicyGradientNetwork to interact with the OpenAI environment, which contains\n",
        "* Model architecture\n",
        "* Model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ll9xqd097UK"
      },
      "source": [
        "class PolicyGradientNetwork(tf.keras.Model):\n",
        "  \"\"\"A policy gradient based reinformcement learning network.\"\"\"\n",
        "  def __init__(self, \n",
        "               state_size: int,\n",
        "               num_actions: int,\n",
        "               hidden_size: int):\n",
        "    \"\"\"Initializes the network architecture.\n",
        "\n",
        "    Args:\n",
        "      state_size: the number of values in a state.\n",
        "      num_actions: the number of actions that the agent can take.\n",
        "      hidden_size: the number of nodes in a hidden layer.\n",
        "    \"\"\"\n",
        "    super(PolicyGradientNetwork, self).__init__()\n",
        "    self.model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(hidden_size, activation='relu'),\n",
        "      tf.keras.layers.Dense(hidden_size, activation='relu'),\n",
        "      tf.keras.layers.Dense(num_actions, activation='softmax')\n",
        "    ])\n",
        "\n",
        "  def call(self, state: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Forward propogates input through the network. \n",
        "\n",
        "    Args: \n",
        "      state: the current state of the agent of shape (num_actions,).\n",
        "\n",
        "    Returns:\n",
        "      The output Tensor of shape (num_actions,).\n",
        "    \"\"\"\n",
        "    output = self.model(tf.convert_to_tensor(state))\n",
        "    return output"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBUd7rEXQUW_"
      },
      "source": [
        "## Sampling Actions\n",
        "* Samples an action based on the model’s action probability distribution\n",
        "* Supports greedy, e-greedy, annealed e-greedy \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAJG5yfV9qOb"
      },
      "source": [
        "def epsilon_greedy_action(action_distribution: tf.Tensor, \n",
        "                          epsilon: float=1e-1) -> int:\n",
        "  \"\"\"Picks an action based on e-greedy policy.\n",
        "\n",
        "  Args:\n",
        "    action_distribution: the agent's action distribution of shape\n",
        "      (num_actions,).\n",
        "    epsilon: a probability that decides whether to select an action randomly or \n",
        "      not.\n",
        "\n",
        "  Returns: \n",
        "    the index of the action to be taken.\n",
        "  \"\"\"\n",
        "  if random.random() < epsilon:\n",
        "    return np.argmax(np.random.random(action_distribution.shape))\n",
        "  else:\n",
        "    return np.argmax(action_distribution)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PKbr2Lx92IK"
      },
      "source": [
        "def epsilon_greedy_action_annealed(action_distribution: tf.Tensor,\n",
        "                                   percentage: float,\n",
        "                                   epsilon_start: float=1.0,\n",
        "                                   epsilon_end: float=1e-2) -> int:\n",
        "  \"\"\"Picks an action based on annealed e-greedy policy.\n",
        "  \n",
        "  Args: \n",
        "    action_distribution: the agent's action distribution of shape \n",
        "      (num_actions,).\n",
        "    percentage: a probability between 0 and 1 that decides the progress of \n",
        "      epsilon change.\n",
        "    epsilon_start: the starting probability that decides whether to select an \n",
        "      action randomly or not.\n",
        "    epsilon_end: the end probability that decides whether to select an action \n",
        "      randomly or not.\n",
        "\n",
        "  Returns: \n",
        "    the index of the action to be taken.\n",
        "  \"\"\"\n",
        "  annealed_epsilon = epsilon_start * (1.0 - percentage) + \\\n",
        "                     epsilon_end * percentage\n",
        "  if random.random() < annealed_epsilon:\n",
        "    return np.argmax(np.random.random(action_distribution.shape))\n",
        "  else:\n",
        "    return np.argmax(action_distribution)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxnx_JNTVDoI"
      },
      "source": [
        "def predict_action(action_distribution: tf.Tensor, \n",
        "                   epsilon_percentage: float, \n",
        "                   explore_exploit_setting:str=\n",
        "                    'epsilon_greedy_annealed_1.0->0.001') -> int:\n",
        "  \"\"\"Chooses an action based on the action probability distribution and an \n",
        "  explore vs. exploit policy.\n",
        "\n",
        "  Args:\n",
        "    action_distribution: the agent's action distribution of shape \n",
        "      (num_actions,).\n",
        "    epsilon_percentage: a probability that decides the progress of epsilon \n",
        "      change.\n",
        "    explore_exploit_setting: defines the action selection policy.\n",
        "\n",
        "  Returns:\n",
        "    the index of the action to be taken.\n",
        "  \"\"\"\n",
        "  if explore_exploit_setting == 'greedy':\n",
        "    action = epsilon_greedy_action(action_distribution)\n",
        "  elif explore_exploit_setting == 'epsilon_greedy_0.05':\n",
        "    action = epsilon_greedy_action(action_distribution, 0.05)\n",
        "  elif explore_exploit_setting == 'epsilon_greedy_0.25':\n",
        "    action = epsilon_greedy_action(action_distribution, 0.25)\n",
        "  elif explore_exploit_setting == 'epsilon_greedy_0.50':\n",
        "    action = epsilon_greedy_action(action_distribution, 0.50)\n",
        "  elif explore_exploit_setting == 'epsilon_greedy_0.90':\n",
        "    action = epsilon_greedy_action(action_distribution, 0.90)\n",
        "  elif explore_exploit_setting == 'epsilon_greedy_annealed_1.0->0.001':\n",
        "    action = epsilon_greedy_action_annealed(action_distribution, \n",
        "                                            epsilon_percentage, \n",
        "                                            1.0, \n",
        "                                            0.001)\n",
        "  elif explore_exploit_setting == 'epsilon_greedy_annealed_0.5->0.001':\n",
        "    action = epsilon_greedy_action_annealed(action_distribution, \n",
        "                                            epsilon_percentage, \n",
        "                                            0.5, \n",
        "                                            0.001)\n",
        "  elif explore_exploit_setting == 'epsilon_greedy_annealed_0.25->0.001':\n",
        "    action = epsilon_greedy_action_annealed(action_distribution, \n",
        "                                            epsilon_percentage, \n",
        "                                            0.25, \n",
        "                                            0.001)\n",
        "  return action"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQdsZf_3Z-fc"
      },
      "source": [
        "## Defining Policy Gradient Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diNnVUNEPY2T"
      },
      "source": [
        "def policy_gradient_loss(outputs: tf.Tensor, \n",
        "                         actions: tf.Tensor, \n",
        "                         rewards: tf.Tensor) -> float:\n",
        "  \"\"\"Computes the policy gradient loss.\n",
        "\n",
        "  Args: \n",
        "    outputs: the forward propogation output of shape (num_actions,).\n",
        "    actions: the agent actions taken of shape (update_frequency,).\n",
        "    rewards: the agent rewards of shape (update_frequency,).\n",
        "\n",
        "  Returns:\n",
        "    the policy gradient loss.\n",
        "  \"\"\"\n",
        "  indices = tf.range(0, tf.shape(outputs)[0]) * tf.shape(outputs)[1] + actions\n",
        "  responsible_outputs = tf.gather(tf.reshape(outputs, [-1]), indices)\n",
        "  loss = -tf.reduce_mean(tf.math.log(responsible_outputs) * rewards)\n",
        "  return loss"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKDEk3BuQK-s"
      },
      "source": [
        "def train_step(model: Callable, \n",
        "               states: tf.Tensor, \n",
        "               actions: tf.Tensor, \n",
        "               rewards: tf.Tensor) -> float:\n",
        "  \"\"\"Performs policy gradient update for a given minibatch.\n",
        "\n",
        "  Args:\n",
        "    model: a callable network to perform forward progagation.\n",
        "    states: the agent states of shape (state_size,) in a given minibath.\n",
        "    actions: the agent actions taken of shape (update_frequency,).\n",
        "    rewards: the agent rewards of shape (update_frequency,).\n",
        "\n",
        "  Returns:\n",
        "    the policy gradient loss.\n",
        "  \"\"\"\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = model(states)\n",
        "    loss = policy_gradient_loss(outputs, actions, rewards)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxJ1F01XaJM2"
      },
      "source": [
        "## Using Discounted Rewards\n",
        "Penalize for rewards that's taking more time steps by a factor of gamma per time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpEEAYI_ok4K"
      },
      "source": [
        "def discount_rewards(rewards: np.ndarray, \n",
        "                     gamma=0.98) -> np.ndarray:\n",
        "  \"\"\"Computes the discounted rewards by a factor of gamma.\n",
        "  \n",
        "  Args:\n",
        "    rewards: rewards in a full episode.\n",
        "    gamma: discount factor over time step.\n",
        "\n",
        "  Returns:\n",
        "    the discounted rewards.\n",
        "  \"\"\"\n",
        "  discounted_returns = [0 for _ in rewards]\n",
        "  discounted_returns[-1] = rewards[-1]\n",
        "  for t in range(len(rewards) - 2, -1, -1):  # iterate backwards\n",
        "      discounted_returns[t] = rewards[t] + discounted_returns[t + 1] * gamma\n",
        "  return discounted_returns"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GCQ-8iFQkXE"
      },
      "source": [
        "## Keeping Track of History\n",
        "Gradients are aggregated from multiple episodes, so it’s necessary to keep track of (state, action, reward) tuples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K_dd4mL9-N2"
      },
      "source": [
        "class EpisodeHistory(object):\n",
        "  \"\"\"Records an episode's (state, action, reward, discounted_returns) tuples.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "    self.discounted_returns = []\n",
        "\n",
        "  def add_to_history(self, \n",
        "                     state: np.ndarray, \n",
        "                     action: int, \n",
        "                     reward: float):\n",
        "    \"\"\"Appends each element to their corresponding lists, called per time step.\n",
        "\n",
        "    Args: \n",
        "      state: the current state of the agent of shape (state_size,).\n",
        "      action: the index of the action taken by the agent.\n",
        "      reward: the reward get by taking this action\n",
        "    \"\"\"\n",
        "    self.states.append(state)\n",
        "    self.actions.append(action)\n",
        "    self.rewards.append(reward)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbdg3Gqf-AkB"
      },
      "source": [
        "class Memory(object):\n",
        "  \"\"\"Records flattened (state, action, reward, discounted_reward) tuples across \n",
        "  all episodes in the minibatch.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "    self.discounted_returns = []\n",
        "\n",
        "  def reset_memory(self):\n",
        "    \"\"\"Clears memory once a policy gradient descent is done on a minibatch.\"\"\"\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "    self.discounted_returns = []\n",
        "\n",
        "  def add_episode(self, episode: EpisodeHistory):\n",
        "    \"\"\"Appends each element in an episode to the corresponding list in memory.\n",
        "    \n",
        "    Args:\n",
        "      episode: (state, action, reward, discounted_returns) tuples in an episode.\n",
        "    \"\"\"\n",
        "    self.states += episode.states\n",
        "    self.actions += episode.actions\n",
        "    self.rewards += episode.rewards\n",
        "    self.discounted_returns += episode.discounted_returns"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykYgdXefQq-I"
      },
      "source": [
        "## Policy Gradient Main Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZjsnEF7-FUu",
        "outputId": "ef664a76-3f81-4bb5-b7dd-d2c5e04e72ce"
      },
      "source": [
        "# Configure Settings\n",
        "total_episodes = 5000      # number of independent episodes to train\n",
        "epsilon_stop = 3000        # the max episode to increase the epsilon in annealed \n",
        "                           # e-greedy policy\n",
        "update_frequency = 8       # number of episodes to update model parameters\n",
        "max_episode_length = 500   # limit a single episode to be of finite length\n",
        "learning_rate = 0.01\n",
        "should_render = False\n",
        "explore_exploit_setting = 'epsilon_greedy_annealed_1.0->0.001'\n",
        "env = gym.make('CartPole-v0')\n",
        "state_size = env.observation_space.shape[0]  # 4 for CartPole-v0\n",
        "num_actions = env.action_space.n             # 2 for CartPole-v0\n",
        "solved = False\n",
        "\n",
        "# Declare Policy Gradient Networks\n",
        "policy_gradient_model = PolicyGradientNetwork(state_size=state_size,\n",
        "                                              num_actions=num_actions,\n",
        "                                              hidden_size=16)\n",
        "optimizer = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "episode_rewards = []\n",
        "batch_losses = []\n",
        "global_memory = Memory()\n",
        "steps = 0\n",
        "\n",
        "# Start training\n",
        "for i in range(total_episodes):\n",
        "  state = env.reset()\n",
        "  episode_reward = 0.0\n",
        "  episode_history = EpisodeHistory()\n",
        "  epsilon_percentage = float(min(i / float(epsilon_stop), 1.0))\n",
        "\n",
        "  # Run a single episode\n",
        "  for j in range(max_episode_length):\n",
        "    # Probabilistically pick an action given our network output\n",
        "    action_distribution = policy_gradient_model([state])\n",
        "    action = predict_action(action_distribution, epsilon_percentage)\n",
        "    # Get reward and next state\n",
        "    next_state, reward, terminal, _ = env.step(action)\n",
        "\n",
        "    if should_render:\n",
        "      env.render()\n",
        "\n",
        "    episode_history.add_to_history(state, action, reward)\n",
        "    state = next_state\n",
        "    episode_reward += reward\n",
        "    steps += 1\n",
        "    \n",
        "    if terminal:\n",
        "      # discounted_returns is updated separately only once every full episode, \n",
        "      # computed from the rewards.\n",
        "      episode_history.discounted_returns = \\\n",
        "        discount_rewards(episode_history.rewards)\n",
        "    \n",
        "      global_memory.add_episode(episode_history)\n",
        "      \n",
        "      # Update model parameters every update_frequency episodes\n",
        "      if i % update_frequency == 0 and i != 0:\n",
        "        batch_loss = train_step(policy_gradient_model, \n",
        "                                global_memory.states, \n",
        "                                global_memory.actions, \n",
        "                                global_memory.discounted_returns)\n",
        "        batch_losses.append(batch_loss)\n",
        "        global_memory.reset_memory()\n",
        "\n",
        "      episode_rewards.append(episode_reward)\n",
        "      break\n",
        "\n",
        "  # The problem is solved if per episode reward is greater than 100\n",
        "  if i % 10 and i != 0:\n",
        "    if np.mean(episode_rewards[:-100]) > 100.0:\n",
        "      solved = True\n",
        "\n",
        "  # Print last 100 episode's mean score\n",
        "  if i % 100 == 0 and i != 0:\n",
        "    print(\"Averaged rewards:\", np.mean(episode_rewards[:-100]))\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Averaged rewards: 29.0\n",
            "Averaged rewards: 21.346534653465348\n",
            "Averaged rewards: 22.35323383084577\n",
            "Averaged rewards: 23.524916943521596\n",
            "Averaged rewards: 25.00498753117207\n",
            "Averaged rewards: 26.65868263473054\n",
            "Averaged rewards: 28.41098169717138\n",
            "Averaged rewards: 29.823109843081312\n",
            "Averaged rewards: 32.161048689138575\n",
            "Averaged rewards: 34.283018867924525\n",
            "Averaged rewards: 36.603396603396604\n",
            "Averaged rewards: 39.01634877384196\n",
            "Averaged rewards: 41.823480432972524\n",
            "Averaged rewards: 45.562644119907766\n",
            "Averaged rewards: 49.43540328336902\n",
            "Averaged rewards: 54.01532311792138\n",
            "Averaged rewards: 59.04122423485322\n",
            "Averaged rewards: 64.19870664315108\n",
            "Averaged rewards: 68.89838978345364\n",
            "Averaged rewards: 74.62125197264598\n",
            "Averaged rewards: 79.65717141429285\n",
            "Averaged rewards: 85.00047596382674\n",
            "Averaged rewards: 90.12812358019082\n",
            "Averaged rewards: 94.89265536723164\n",
            "Averaged rewards: 99.22157434402332\n",
            "Averaged rewards: 103.23390643742503\n",
            "Averaged rewards: 106.93848519800076\n",
            "Averaged rewards: 110.3802295446131\n",
            "Averaged rewards: 113.57979293109604\n",
            "Averaged rewards: 116.55877283695277\n",
            "Averaged rewards: 119.33922025991336\n",
            "Averaged rewards: 121.94034182521767\n",
            "Averaged rewards: 124.37894407997501\n",
            "Averaged rewards: 126.66979703120266\n",
            "Averaged rewards: 128.8259335489562\n",
            "Averaged rewards: 130.85889745786918\n",
            "Averaged rewards: 132.77895029158566\n",
            "Averaged rewards: 134.59524452850582\n",
            "Averaged rewards: 136.31596948171534\n",
            "Averaged rewards: 137.9484747500641\n",
            "Averaged rewards: 139.49937515621096\n",
            "Averaged rewards: 140.97464033162643\n",
            "Averaged rewards: 142.3796715067841\n",
            "Averaged rewards: 143.7193675889328\n",
            "Averaged rewards: 144.99818223131106\n",
            "Averaged rewards: 146.22017329482338\n",
            "Averaged rewards: 147.38904585959574\n",
            "Averaged rewards: 148.50818974686237\n",
            "Averaged rewards: 149.5807123515934\n"
          ]
        }
      ]
    }
  ]
}